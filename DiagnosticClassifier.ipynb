{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import textwrap\n",
    "import re\n",
    "import pynlpir\n",
    "pynlpir.open()\n",
    "from sklearn import metrics\n",
    "from scipy import stats\n",
    "import scipy as sp\n",
    "\n",
    "from helpers import *\n",
    "from generate import *\n",
    "from diagnostic import *\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis on linux source code language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity LSTM: 2.18350716101\n",
      "Perplexity GRU:  2.29313456059\n"
     ]
    }
   ],
   "source": [
    "model_lstm = torch.load('models/linux_3x512_0d3_lstm_200l_40000E.model').cuda()\n",
    "model_gru = torch.load('models/linux_3x512_0d3_gru_200l_40000E.model').cuda()\n",
    "print('Perplexity LSTM:', 2**np.mean([test_model(model_lstm, 'data/linux/test.txt') for _ in range(1)]))\n",
    "print('Perplexity GRU: ', 2**np.mean([test_model(model_gru, 'data/linux/test.txt') for _ in range(1)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============== LSTM MODEL ==============\n",
      "\n",
      "struct intel_gpio_ctrl {\n",
      "\tunsigned long flags;\n",
      "\tstruct gpio_dev *gspi = &fsm_gpio_get_gpio_desc(althatout);\n",
      "\tunsigned long flags = 0;\n",
      "\n",
      "\tdata = av_voltage_gpio;\n",
      "\n",
      "\tctrls->gfx = cpu_to_be32(0x72);\n",
      "\tREG_FUNCTION(0xc, \"gpio-fmc0\", 0, RT8769_OFFSET_4_MASK),\n",
      "\t\tV4L2_CID_BANK_FIELD(VI4_F12, VV03AD_REG_CTRL_VSYNC_NUM_B, 7);\n",
      "\tVGA_DS1680(VGA_VALUE_TO_LDON, vlan_vbase);\n",
      "#endif\n",
      "}\n",
      "\n",
      "static void card9970_write(void *arg)\n",
      "{\n",
      "\tstruct v4l2_subdev *sd = v4l2_get_drvdata(chip->drvdata);\n",
      "\tstruct v4l2_subdev *sd = &vga-\n",
      "\n",
      "============== GRU MODEL ===============\n",
      "\n",
      "static int clock_device_clk_disable(struct unarchdev_int *advh,\n",
      "\t\t\t\t u32 active_segment)\n",
      "{\n",
      "\tstruct intel_enqueue *adapter;\n",
      "\tstruct irq_data *data = dev_get_drvdata(dev);\n",
      "\tstruct max3990_device *dev = adapter->hw;\n",
      "\tint ret;\n",
      "\n",
      "\tret = 0;\n",
      "\n",
      "\tif (regs->config == LXCHIP_DATA_IRQ_MASK)\n",
      "\t\treturn;\n",
      "\n",
      "\tcs_scan_channel(chip->chip_ade);\n",
      "\tdcbx_write(client, mcrs, DC_8XX_COMMATE_DIT, map, &clk);\n",
      "\n",
      "\t/*\n",
      "\t * Locking must be send because the pci requests.\n",
      "\t */\n",
      "\tif ((skb_xmit_mask & MMC_MSI_MAX_MAX)) {\n",
      "\t\trc = hbm_set_c\n"
     ]
    }
   ],
   "source": [
    "print('\\n============== LSTM MODEL ==============\\n')\n",
    "text, hiddens = generate(model_lstm, '\\n\\n', 500, 0.8, True)\n",
    "print(text)\n",
    "\n",
    "print('\\n============== GRU MODEL ===============\\n')\n",
    "text, hiddens = generate(model_gru, '\\n\\n', 500, 0.8, True)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Code Hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def hypothesis_inlinecounter(text):\n",
    "    hyp = np.concatenate([np.linspace(1, -1, len(x)+1) for x in text.split('\\n')])[:-1]\n",
    "    return hyp\n",
    "\n",
    "def hypothesis_inside_one(text, single):\n",
    "    hyp = re.sub('\\{}.*?\\{}'.format(single, single), lambda m: single+'#'*(len(m.group())-2)+single, text)\n",
    "    return np.array([1 if x == '#' else -1 for x in hyp])\n",
    "\n",
    "def hypothesis_inside_two(text, left, right):\n",
    "    hyp = np.full(len(text), -1)\n",
    "    inside = False\n",
    "    for i in range(len(text) - 1):\n",
    "        if text[i] == left:\n",
    "            inside = True\n",
    "        elif text[i] == right:\n",
    "            inside = False\n",
    "        if inside:\n",
    "            hyp[i+1] = 1\n",
    "    return hyp\n",
    "\n",
    "hypothesis_inside_quotation = lambda x: hypothesis_inside_one(x, '\"')\n",
    "hypothesis_inside_parantheses = lambda x: hypothesis_inside_two(x, '(', ')')\n",
    "\n",
    "def hypothesis_comments(text):\n",
    "    hyp = np.full(len(text), -1)\n",
    "    in_brac_comment = False\n",
    "    in_line_comment = False\n",
    "    for i in range(len(text)):\n",
    "        if text[i:i+2] == '//':\n",
    "            in_line_comment = True\n",
    "        elif text[i] == '\\n':\n",
    "            in_line_comment = False\n",
    "        elif text[i:i+2] == '/*':\n",
    "            in_brac_comment = True\n",
    "        elif text[i:i+2] == '*/':\n",
    "            in_brac_comment = False\n",
    "        if in_brac_comment:\n",
    "            hyp[i:i+3] = 1\n",
    "        if in_line_comment:\n",
    "            hyp[i:i+1] = 1\n",
    "    return hyp\n",
    "\n",
    "def hypothesis_indentation(text, level):\n",
    "    hyp = np.full(len(text), -1)\n",
    "    cur_level = 0\n",
    "    for i, char in enumerate(text):\n",
    "        if char == '\\n':\n",
    "            cur_level = 0\n",
    "        elif char == '\\t':\n",
    "            cur_level += 1\n",
    "        if cur_level >= level:\n",
    "            hyp[i] = 1\n",
    "    return hyp\n",
    "# plot_colored_text(text, hypothesis_inlinecounter(text), title='Hypothesis: Inline counter', save_file='plots/hyp_inline_counter.png')\n",
    "# plot_colored_text(text, hypothesis_inside_quotation(text), title='Hypothesis: Inside quotation', save_file='plots/hyp_inside_quotation.png')\n",
    "# plot_colored_text(text, hypothesis_inside_parantheses(text), title='Hypothesis: Inside parantheses', save_file='plots/hyp_inside_parantheses.png')\n",
    "# plot_colored_text(text, hypothesis_comments(text), title='Hypothesis: Comments', save_file='plots/hyp_comments.png')\n",
    "# plot_colored_text(text, hypothesis_indentation(text, 1), title='Hypothesis: Indent level 1', save_file='plots/hyp_indent_1.png')\n",
    "# plot_colored_text(text, hypothesis_indentation(text, 2), title='Hypothesis: Indent level 2', save_file='plots/hyp_indent_2.png')\n",
    "# plot_colored_text(text, hypothesis_indentation(text, 3), title='Hypothesis: Indent level 3', save_file='plots/hyp_indent_3.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def full_test(model, hypothesis, name, plot=False, train_len=95, test_len=10,\n",
    "              ex_name='test'):\n",
    "    y_true, y_pred = validate_hypothesis(model, LogisticRegression(), hypothesis,\n",
    "                                         train_len=train_len, test_len=train_len,\n",
    "                                         save_hyp='plots/hyp_{}.png'.format(ex_name),\n",
    "                                         save_diag='plots/diag_{}.png'.format(ex_name),\n",
    "                                         save_resp='plots/resp_{}.png'.format(ex_name))\n",
    "    metric_pearsonr = lambda a, b: stats.pearsonr(a, b)[0]\n",
    "    \n",
    "    print(\"Hypothesis: {} (normal)\".format(name))\n",
    "    print('acc:      ', metrics.accuracy_score(y_true, y_pred))\n",
    "    print('prec:     ', metrics.precision_score(y_true, y_pred))\n",
    "    print('recall:   ', metrics.recall_score(y_true, y_pred))\n",
    "    print('f1-score: ', metrics.f1_score(y_true, y_pred))\n",
    "    print('pearsonr: ', metric_pearsonr(y_true, y_pred))\n",
    "    y_true, y_pred = validate_hypothesis(model, LogisticRegression(class_weight='balanced'),\n",
    "                                         hypothesis, train_len=train_len, test_len=test_len,\n",
    "                                         save_hyp='plots/hyp_{}_balanced.png'.format(ex_name),\n",
    "                                         save_diag='plots/diag_{}_balanced.png'.format(ex_name),\n",
    "                                         save_resp='plots/resp_{}_balanced.png'.format(ex_name))\n",
    "    print(\"Hypothesis: {} (balanced)\".format(name))\n",
    "    print('acc:      ', metrics.accuracy_score(y_true, y_pred))\n",
    "    print('prec:     ', metrics.precision_score(y_true, y_pred))\n",
    "    print('recall:   ', metrics.recall_score(y_true, y_pred))\n",
    "    print('f1-score: ', metrics.f1_score(y_true, y_pred))\n",
    "    print('pearsonr: ', metric_pearsonr(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for model_name, model in [('linux_lstm', model_lstm), ('linux_gru', model_gru)]:\n",
    "    print(\"MODEL: \", model_name)\n",
    "    y_true, y_pred = validate_hypothesis(model, LinearRegression(), hypothesis_inlinecounter,\n",
    "                                         train_len=95, test_len=1,\n",
    "                                         save_hyp='plots/{}_hyp_inlinecounter.png'.format(model_name),\n",
    "                                         save_diag='plots/{}_diag_inlinecounter.png'.format(model_name),\n",
    "                                         save_resp='plots/{}_resp_inlinecounter.png'.format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL:  linux_lstm\n"
     ]
    }
   ],
   "source": [
    "for model_name, model in [('linux_lstm', model_lstm), ('linux_gru', model_gru)]:\n",
    "    print(\"MODEL: \", model_name)\n",
    "    full_test(model, hypothesis_inside_quotation, 'Inside Quotation',\n",
    "              train_len=95, test_len=10, ex_name='inside_quotation'.format(model_name))\n",
    "    full_test(model, hypothesis_comments, 'Comments',\n",
    "              train_len=95, test_len=10, ex_name='{}_inside_comments'.format(model_name))\n",
    "    full_test(model, lambda x: hypothesis_indentation(x, 1), 'Indentation level 1',\n",
    "              train_len=95, test_len=10, ex_name='{}_inside_indent_1'.format(model_name))\n",
    "    full_test(model, lambda x: hypothesis_indentation(x, 2), 'Indentation level 2',\n",
    "              train_len=95, test_len=10, ex_name='{}_inside_indent_2'.format(model_name))\n",
    "    full_test(model, lambda x: hypothesis_indentation(x, 3), 'Indentation level 3',\n",
    "              train_len=95, test_len=10, ex_name='{}_inside_indent_3'.format(model_name))\n",
    "    full_test(model, hypothesis_inside_parantheses, 'Inside Parantheses',\n",
    "              train_len=95, test_len=10, ex_name='{}_inside_parantheses'.format(model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis on Shakespeare model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Shakespeare language odel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_gru = torch.load('models/simple.model').cuda()\n",
    "print('Perplexity:', 2**np.mean([test_model(model_gru, 'data/tiny-shakespeare/test.txt') for _ in range(1)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print('\\n============== LSTM MODEL ==============\\n')\n",
    "# text, hiddens = generate(model_lstm, '\\n\\n', 500, 0.8, True)\n",
    "# print(text)\n",
    "\n",
    "print('\\n============== GRU MODEL ===============\\n')\n",
    "text, hiddens = generate(model_gru, '\\n\\n', 500, 0.8, True)\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define language hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hypothesis_inlinecounter(text):\n",
    "    hyp = np.concatenate([np.linspace(1, -1, len(x)+1) for x in text.split('\\n')])[:-1]\n",
    "    return hyp\n",
    "\n",
    "def hypothesis_capswords(text):\n",
    "    hyp = np.concatenate([np.full(len(x)+1, 1) if re.sub('[^a-zA-Z]+', '', x).isupper() else np.full(len(x)+1, -1) for x in text.split('\\n')])[:-1]\n",
    "    return hyp\n",
    "\n",
    "def hypothesis_pos(text, pos_tag):\n",
    "    hyp = text.replace('1', '0')\n",
    "    for word, tag in pynlpir.segment(text):\n",
    "        if tag == pos_tag:\n",
    "            hyp = hyp.replace(word, '1'*len(word), 1)\n",
    "        else:\n",
    "            hyp = hyp.replace(word, '0'*len(word), 1)\n",
    "    hyp = [1 if x=='1' else -1 for x in re.sub('[^1]', '0', hyp)]\n",
    "    \n",
    "    return hyp\n",
    "\n",
    "def hypothesis_verbs(text):\n",
    "    return hypothesis_pos(text, 'verb')\n",
    "\n",
    "def hypothesis_nouns(text):\n",
    "    return hypothesis_pos(text, 'noun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot_colored_text(text, hypothesis_inlinecounter(text), title='Hypothesis: Inline Counter', save_file='plots/hyp_inlinecounter.png')\n",
    "# plot_colored_text(text, hypothesis_capswords(text), title='Hypothesis: Capitalized Words', save_file='plots/hyp_capswords.png')\n",
    "# plot_colored_text(text, hypothesis_verbs(text), title='Hypothesis: Verbs', save_file='plots/hyp_verbs')\n",
    "# plot_colored_text(text, hypothesis_nouns(text), title='Hypothesis: Nouns', save_file='plots/hyp_nouns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate hypothesis data\n",
    "def gen_hyp_data(model, N, text_len=500):\n",
    "    texts, hiddens, hyps = [], [], []\n",
    "    for i in range(N):\n",
    "        text, hidden = generate(model, '\\n\\n', text_len, 0.8, True)\n",
    "        hidden = hidden.reshape(hidden.shape[0], -1)\n",
    "        hyp = hypothesis_inlinecounter(text)\n",
    "        hiddens.append(hidden)\n",
    "        hyps.append(hyp)\n",
    "        texts.append(text)\n",
    "    return ''.join(texts), np.concatenate(hyps), np.concatenate(hiddens)\n",
    "\n",
    "# Generate train and test data\n",
    "train_texts, train_hyps, train_hiddens = gen_hyp_data(model_gru, 500)\n",
    "test_texts, test_hyps, test_hiddens = gen_hyp_data(model_gru, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diag_classifier = LinearRegression()\n",
    "# Train Diagnostic Classifier\n",
    "diag_classifier.fit(train_hiddens, train_hyps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.argsort(np.abs(diag_classifier.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scipy.stats.pearsonr(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find responsible neuron\n",
    "resp_neuron = np.argmax(np.abs(diag_classifier.coef_))\n",
    "print(resp_neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in np.argsort(np.abs(diag_classifier.coef_))[-10:]:\n",
    "    plot_colored_text(train_texts[:500], train_hiddens[:500, i],\n",
    "                      title='Most Responsible Neuron {}'.format(i),\n",
    "                      save_file='plots/temp_{}.png'.format(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(200):\n",
    "    plot_colored_text(train_texts[:500], train_hiddens[:500, i],\n",
    "                      title='Most Responsible Neuron {}'.format(i),\n",
    "                      save_file='plots/temp_{}.png'.format(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_true, y_pred = validate_hypothesis(model, LinearRegression(), hypothesis_inlinecounter,\n",
    "                                     train_len=95, test_len=5,\n",
    "                                     save_hyp='plots/hyp_inlinecounter_shake.png',\n",
    "                                     save_diag='plots/diag_inlinecounter_shake.png',\n",
    "                                     save_resp='plots/resp_inlinecounter_shake.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_test(model, hypothesis_capswords, 'Capitalized Words',\n",
    "          train_len=95, test_len=5, name='capswords')\n",
    "full_test(model, hypothesis_nouns, 'Nouns',\n",
    "          train_len=95, test_len=5, name='nouns')\n",
    "full_test(model, hypothesis_verbs, 'Verbs',\n",
    "          train_len=95, test_len=5, name='verbs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
