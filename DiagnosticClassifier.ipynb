{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import textwrap\n",
    "import re\n",
    "import pynlpir\n",
    "pynlpir.open()\n",
    "from sklearn import metrics\n",
    "from scipy import stats\n",
    "import scipy as sp\n",
    "\n",
    "from helpers import *\n",
    "from generate import *\n",
    "from diagnostic import *\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis on linux source code language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tycho/anaconda3/lib/python3.6/site-packages/torch/serialization.py:316: SourceChangeWarning: source code of class 'torch.nn.modules.sparse.Embedding' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/tycho/anaconda3/lib/python3.6/site-packages/torch/serialization.py:316: SourceChangeWarning: source code of class 'torch.nn.modules.rnn.LSTM' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/tycho/anaconda3/lib/python3.6/site-packages/torch/serialization.py:316: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/tycho/anaconda3/lib/python3.6/site-packages/torch/serialization.py:316: SourceChangeWarning: source code of class 'torch.nn.modules.sparse.Embedding' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/tycho/anaconda3/lib/python3.6/site-packages/torch/serialization.py:316: SourceChangeWarning: source code of class 'torch.nn.modules.rnn.GRU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/tycho/anaconda3/lib/python3.6/site-packages/torch/serialization.py:316: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity LSTM: 2.26045721122\n",
      "Perplexity GRU:  2.30455089006\n"
     ]
    }
   ],
   "source": [
    "model_lstm = torch.load('models/linux_3x512_0d3_lstm_200l_40000E.model').cuda()\n",
    "model_gru = torch.load('models/linux_3x512_0d3_gru_200l_40000E.model').cuda()\n",
    "print('Perplexity LSTM:', 2**np.mean([test_model(model_lstm, 'data/linux/test.txt') for _ in range(1)]))\n",
    "print('Perplexity GRU: ', 2**np.mean([test_model(model_gru, 'data/linux/test.txt') for _ in range(1)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============== LSTM MODEL ==============\n",
      "\n",
      "tatic void atomisp_css_hs_enable_attributes(struct sock *sk)\n",
      "{\n",
      "\tint err;\n",
      "\n",
      "\tswitch (osd_req->chanspec) {\n",
      "\tcase SYS_CTL_DAC_OP:\n",
      "\t\tif (status & APD_SOT_SUPPORTED) {\n",
      "\t\t\tif (sg->soft_soft_state != ASIC_IC_OPTION) {\n",
      "\t\t\t\tsg_send_msg(as, state->soft_seq, mask, sg_size_entry);\n",
      "\t\t\t\tsnd_soc_update_bits(sdc, cycles_state, skb);\n",
      "\n",
      "\t\t\t\tspin_unlock(&dss->set_ssid_lock);\n",
      "\t\t\t\tssa_port_set_current_dump(support_start, SSP_SRC_UNKNOWN);\n",
      "\t\t\t\tsuccess_ops->soc_ops(sk, SAS_OK);\n",
      "\t\t\t\tmsg_set_speed_base(asus->sdma_cpu, SAS\n",
      "\n",
      "============== GRU MODEL ===============\n",
      "\n",
      "if (dev->irq == NULL) {\n",
      "\t\tpr_err(\"%s: failed to read this reset\\n\");\n",
      "\t\treturn;\n",
      "\t}\n",
      "\n",
      "\t/* hardware colorspace in State */\n",
      "\tif (action) {\n",
      "\t\tfor (i = 0; i < 1000;\n",
      "}\n",
      "\n",
      "static void atmel_walk_state_transaction(struct sk_buff *header)\n",
      "{\n",
      "\tint rc = 0;\n",
      "\n",
      "\t/* Check if the static values */\n",
      "\tif (cpuid_connect < 0 || chpi->base_state == QLA82XX_DIG_REQUEST_ATTR_FLAG_LOAD)\n",
      "\t\tcs->enable_idx = 1;\n",
      "\telse\n",
      "\t\taddr |= HIDMAV_IN_TX_DMA_CONFIGURED;\n",
      "\tif (!(mode & MISC_REG_DISPLAY_ENCODE)) {\n",
      "\t\tif (priv->cma_enabled & (ct_ena\n"
     ]
    }
   ],
   "source": [
    "print('\\n============== LSTM MODEL ==============\\n')\n",
    "text, hiddens = generate(model_lstm, '\\n\\n', 500, 0.8, True)\n",
    "print(text)\n",
    "\n",
    "print('\\n============== GRU MODEL ===============\\n')\n",
    "text, hiddens = generate(model_gru, '\\n\\n', 500, 0.8, True)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Code Hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def hypothesis_inlinecounter(text):\n",
    "    hyp = np.concatenate([np.linspace(1, -1, len(x)+1) for x in text.split('\\n')])[:-1]\n",
    "    return hyp\n",
    "\n",
    "def hypothesis_inside_one(text, single):\n",
    "    hyp = re.sub('\\{}.*?\\{}'.format(single, single), lambda m: single+'#'*(len(m.group())-2)+single, text)\n",
    "    return np.array([1 if x == '#' else -1 for x in hyp])\n",
    "\n",
    "def hypothesis_inside_two(text, left, right):\n",
    "    hyp = np.full(len(text), -1)\n",
    "    inside = False\n",
    "    for i in range(len(text) - 1):\n",
    "        if text[i] == left:\n",
    "            inside = True\n",
    "        elif text[i] == right:\n",
    "            inside = False\n",
    "        if inside:\n",
    "            hyp[i+1] = 1\n",
    "    return hyp\n",
    "\n",
    "hypothesis_inside_quotation = lambda x: hypothesis_inside_one(x, '\"')\n",
    "hypothesis_inside_parantheses = lambda x: hypothesis_inside_two(x, '(', ')')\n",
    "\n",
    "def hypothesis_comments(text):\n",
    "    hyp = np.full(len(text), -1)\n",
    "    in_brac_comment = False\n",
    "    in_line_comment = False\n",
    "    for i in range(len(text)):\n",
    "        if text[i:i+2] == '//':\n",
    "            in_line_comment = True\n",
    "        elif text[i] == '\\n':\n",
    "            in_line_comment = False\n",
    "        elif text[i:i+2] == '/*':\n",
    "            in_brac_comment = True\n",
    "        elif text[i:i+2] == '*/':\n",
    "            in_brac_comment = False\n",
    "        if in_brac_comment:\n",
    "            hyp[i:i+3] = 1\n",
    "        if in_line_comment:\n",
    "            hyp[i:i+1] = 1\n",
    "    return hyp\n",
    "\n",
    "def hypothesis_indentation(text, level):\n",
    "    hyp = np.full(len(text), -1)\n",
    "    cur_level = 0\n",
    "    for i, char in enumerate(text):\n",
    "        if char == '\\n':\n",
    "            cur_level = 0\n",
    "        elif char == '\\t':\n",
    "            cur_level += 1\n",
    "        if cur_level >= level:\n",
    "            hyp[i] = 1\n",
    "    return hyp\n",
    "\n",
    "# plot_colored_text(text, hypothesis_inlinecounter(text), title='Hypothesis: Inline counter', save_file='plots/hyp_inline_counter.png')\n",
    "# plot_colored_text(text, hypothesis_inside_quotation(text), title='Hypothesis: Inside quotation', save_file='plots/hyp_inside_quotation.png')\n",
    "# plot_colored_text(text, hypothesis_inside_parantheses(text), title='Hypothesis: Inside parantheses', save_file='plots/hyp_inside_parantheses.png')\n",
    "# plot_colored_text(text, hypothesis_comments(text), title='Hypothesis: Comments', save_file='plots/hyp_comments.png')\n",
    "# plot_colored_text(text, hypothesis_indentation(text, 1), title='Hypothesis: Indent level 1', save_file='plots/hyp_indent_1.png')\n",
    "# plot_colored_text(text, hypothesis_indentation(text, 2), title='Hypothesis: Indent level 2', save_file='plots/hyp_indent_2.png')\n",
    "# plot_colored_text(text, hypothesis_indentation(text, 3), title='Hypothesis: Indent level 3', save_file='plots/hyp_indent_3.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_test(model, hypothesis, name, plot=False, train_len=95, test_len=10,\n",
    "              ex_name='test'):\n",
    "    y_true, y_pred = validate_hypothesis(model, LogisticRegression(), hypothesis,\n",
    "                                         train_len=train_len, test_len=train_len,\n",
    "                                         save_hyp='plots/hyp_{}.png'.format(ex_name),\n",
    "                                         save_diag='plots/diag_{}.png'.format(ex_name),\n",
    "                                         save_resp='plots/resp_{}.png'.format(ex_name))\n",
    "    metric_pearsonr = lambda a, b: stats.pearsonr(a, b)[0]\n",
    "    \n",
    "    print(\"Hypothesis: {} (normal)\".format(name))\n",
    "    print('acc:      ', metrics.accuracy_score(y_true, y_pred))\n",
    "    print('prec:     ', metrics.precision_score(y_true, y_pred))\n",
    "    print('recall:   ', metrics.recall_score(y_true, y_pred))\n",
    "    print('f1-score: ', metrics.f1_score(y_true, y_pred))\n",
    "    print('pearsonr: ', metric_pearsonr(y_true, y_pred))\n",
    "    y_true, y_pred = validate_hypothesis(model, LogisticRegression(class_weight='balanced'),\n",
    "                                         hypothesis, train_len=train_len, test_len=test_len,\n",
    "                                         save_hyp='plots/hyp_{}_balanced.png'.format(ex_name),\n",
    "                                         save_diag='plots/diag_{}_balanced.png'.format(ex_name),\n",
    "                                         save_resp='plots/resp_{}_balanced.png'.format(ex_name))\n",
    "    print(\"Hypothesis: {} (balanced)\".format(name))\n",
    "    print('acc:      ', metrics.accuracy_score(y_true, y_pred))\n",
    "    print('prec:     ', metrics.precision_score(y_true, y_pred))\n",
    "    print('recall:   ', metrics.recall_score(y_true, y_pred))\n",
    "    print('f1-score: ', metrics.f1_score(y_true, y_pred))\n",
    "    print('pearsonr: ', metric_pearsonr(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_hypothesis(model, diag_classifier, hypothesis, train_len=50,\n",
    "                        test_len=1, text_len=500, temperature=0.8,\n",
    "                        save_hyp=None, save_diag=None, save_resp=None):\n",
    "    # Generate hypothesis data\n",
    "    def gen_hyp_data(model, N, text_len=500):\n",
    "        texts, hiddens, hyps = [], [], []\n",
    "        for i in range(N):\n",
    "            text, hidden = generate(model, '\\n\\n', text_len, temperature, True)\n",
    "            hidden = hidden.reshape(hidden.shape[0], -1)\n",
    "            hyp = hypothesis(text)\n",
    "            hiddens.append(hidden)\n",
    "            hyps.append(hyp)\n",
    "            texts.append(text)\n",
    "        return ''.join(texts), np.concatenate(hyps), np.concatenate(hiddens)\n",
    "\n",
    "    # Generate train and test data\n",
    "    _, train_hyps, train_hiddens = gen_hyp_data(model, train_len)\n",
    "    test_texts, test_hyps, test_hiddens = gen_hyp_data(model, test_len)\n",
    "    print(pearsonr(train_hiddens, train_hyps))\n",
    "    print(pearsonr(test_hiddens, test_hyps))\n",
    "\n",
    "    # Train Diagnostic Classifier\n",
    "    diag_classifier.fit(train_hiddens, train_hyps)\n",
    "    \n",
    "    # Predict with Diagnostic Classifier\n",
    "    pred_hyps = diag_classifier.predict(test_hiddens)\n",
    "    \n",
    "    # Find responsible neuron\n",
    "    resp_neuron = np.argmax(np.abs(diag_classifier.coef_))\n",
    "    print(resp_neuron)\n",
    "    \n",
    "    # Plot results\n",
    "    if save_hyp:\n",
    "        plot_colored_text(test_texts[:text_len], test_hyps[:text_len],\n",
    "                          title='Formed Hypothesis',\n",
    "                          save_file=save_hyp)\n",
    "    if save_diag:\n",
    "        plot_colored_text(test_texts[:text_len], pred_hyps[:text_len],\n",
    "                          title='Diagnostic Classifier Prediction',\n",
    "                          save_file=save_diag)\n",
    "    if save_resp:\n",
    "        plot_colored_text(test_texts[:text_len], test_hiddens[:text_len, resp_neuron],\n",
    "                          title='Most Responsible Neuron {}'.format(resp_neuron),\n",
    "                          save_file=save_resp)\n",
    "        \n",
    "    del(train_hyps)\n",
    "    del(train_hiddens)\n",
    "    del(test_texts)\n",
    "    del(test_hiddens)\n",
    "    gc.collect()\n",
    "    \n",
    "    return test_hyps, pred_hyps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL:  linux_lstm\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (47500,3072) (47500,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-dbcceda99422>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                                          \u001b[0msave_hyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'plots/{}_hyp_inlinecounter.png'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                          \u001b[0msave_diag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'plots/{}_diag_inlinecounter.png'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                                          save_resp='plots/{}_resp_inlinecounter.png'.format(model_name))\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-f5cbcda74261>\u001b[0m in \u001b[0;36mvalidate_hypothesis\u001b[0;34m(model, diag_classifier, hypothesis, train_len, test_len, text_len, temperature, save_hyp, save_diag, save_resp)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_hyps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_hiddens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_hyp_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mtest_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_hyps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_hiddens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_hyp_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpearsonr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_hiddens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_hyps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpearsonr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_hiddens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_hyps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/scipy/stats/stats.py\u001b[0m in \u001b[0;36mpearsonr\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   2999\u001b[0m     \u001b[0mmy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3000\u001b[0m     \u001b[0mxm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mym\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3001\u001b[0;31m     \u001b[0mr_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxm\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mym\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3002\u001b[0m     \u001b[0mr_den\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sum_of_squares\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0m_sum_of_squares\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mym\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3003\u001b[0m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr_num\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mr_den\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (47500,3072) (47500,) "
     ]
    }
   ],
   "source": [
    "for model_name, model in [('linux_lstm', model_lstm), ('linux_gru', model_gru)]:\n",
    "    print(\"MODEL: \", model_name)\n",
    "    y_true, y_pred = validate_hypothesis(model, LinearRegression(), hypothesis_inlinecounter,\n",
    "                                         train_len=95, test_len=1,\n",
    "                                         save_hyp='plots/{}_hyp_inlinecounter.png'.format(model_name),\n",
    "                                         save_diag='plots/{}_diag_inlinecounter.png'.format(model_name),\n",
    "                                         save_resp='plots/{}_resp_inlinecounter.png'.format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for model_name, model in [('linux_lstm', model_lstm), ('linux_gru', model_gru)]:\n",
    "    print(\"MODEL: \", model_name)\n",
    "    full_test(model, hypothesis_inside_quotation, 'Inside Quotation',\n",
    "              train_len=95, test_len=10, ex_name='inside_quotation'.format(model_name))\n",
    "    full_test(model, hypothesis_comments, 'Comments',\n",
    "              train_len=95, test_len=10, ex_name='{}_inside_comments'.format(model_name))\n",
    "    full_test(model, lambda x: hypothesis_indentation(x, 1), 'Indentation level 1',\n",
    "              train_len=95, test_len=10, ex_name='{}_inside_indent_1'.format(model_name))\n",
    "    full_test(model, lambda x: hypothesis_indentation(x, 2), 'Indentation level 2',\n",
    "              train_len=95, test_len=10, ex_name='{}_inside_indent_2'.format(model_name))\n",
    "    full_test(model, lambda x: hypothesis_indentation(x, 3), 'Indentation level 3',\n",
    "              train_len=95, test_len=10, ex_name='{}_inside_indent_3'.format(model_name))\n",
    "    full_test(model, hypothesis_inside_parantheses, 'Inside Parantheses',\n",
    "              train_len=95, test_len=10, ex_name='{}_inside_parantheses'.format(model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis on Shakespeare model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Shakespeare language odel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.load('models/simple.model').cuda()\n",
    "# print('Perplexity:', 2**np.mean([test_model(model, 'data/tiny-shakespeare/test.txt') for _ in range(1)]))\n",
    "model_lstm = torch.load('models/shake_2x128_lstm_3000').cuda()\n",
    "print('Perplexity:', 2**np.mean([test_model(model_lstm, 'data/tiny-shakespeare/test.txt') for _ in range(1)]))\n",
    "model_gru = torch.load('models/shake_2x128_gru_3000').cuda()\n",
    "print('Perplexity:', 2**np.mean([test_model(model_gru, 'data/tiny-shakespeare/test.txt') for _ in range(1)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n============== LSTM MODEL ==============\\n')\n",
    "text, hiddens = generate(model_lstm, '\\n\\n', 500, 0.8, True)\n",
    "print(text)\n",
    "\n",
    "print('\\n============== GRU MODEL ===============\\n')\n",
    "text, hiddens = generate(model_gru, '\\n\\n', 500, 0.8, True)\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define language hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypothesis_inlinecounter(text):\n",
    "    hyp = np.concatenate([np.linspace(1, -1, len(x)+1) for x in text.split('\\n')])[:-1]\n",
    "    return hyp\n",
    "\n",
    "def hypothesis_capswords(text):\n",
    "    hyp = np.concatenate([np.full(len(x)+1, 1) if re.sub('[^a-zA-Z]+', '', x).isupper() else np.full(len(x)+1, -1) for x in text.split('\\n')])[:-1]\n",
    "    return hyp\n",
    "\n",
    "def hypothesis_pos(text, pos_tag):\n",
    "    hyp = text.replace('1', '0')\n",
    "    for word, tag in pynlpir.segment(text):\n",
    "        if tag == pos_tag:\n",
    "            hyp = hyp.replace(word, '1'*len(word), 1)\n",
    "        else:\n",
    "            hyp = hyp.replace(word, '0'*len(word), 1)\n",
    "    hyp = [1 if x=='1' else -1 for x in re.sub('[^1]', '0', hyp)]\n",
    "    \n",
    "    return hyp\n",
    "\n",
    "def hypothesis_verbs(text):\n",
    "    return hypothesis_pos(text, 'verb')\n",
    "\n",
    "def hypothesis_nouns(text):\n",
    "    return hypothesis_pos(text, 'noun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot_colored_text(text, hypothesis_inlinecounter(text), title='Hypothesis: Inline Counter', save_file='plots/hyp_inlinecounter.png')\n",
    "# plot_colored_text(text, hypothesis_capswords(text), title='Hypothesis: Capitalized Words', save_file='plots/hyp_capswords.png')\n",
    "# plot_colored_text(text, hypothesis_verbs(text), title='Hypothesis: Verbs', save_file='plots/hyp_verbs')\n",
    "# plot_colored_text(text, hypothesis_nouns(text), title='Hypothesis: Nouns', save_file='plots/hyp_nouns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate hypothesis data\n",
    "def gen_hyp_data(model, N, text_len=500):\n",
    "    texts, hiddens, hyps = [], [], []\n",
    "    for i in range(N):\n",
    "        text, hidden = generate(model, '\\n\\n', text_len, 0.8, True)\n",
    "        hidden = hidden.reshape(hidden.shape[0], -1)\n",
    "        hyp = hypothesis_inlinecounter(text)\n",
    "        hiddens.append(hidden)\n",
    "        hyps.append(hyp)\n",
    "        texts.append(text)\n",
    "    return ''.join(texts), np.concatenate(hyps), np.concatenate(hiddens)\n",
    "\n",
    "# Generate train and test data\n",
    "train_texts, train_hyps, train_hiddens = gen_hyp_data(model_gru, 500)\n",
    "test_texts, test_hyps, test_hiddens = gen_hyp_data(model_gru, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_classifier = LinearRegression()\n",
    "# Train Diagnostic Classifier\n",
    "diag_classifier.fit(train_hiddens, train_hyps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argsort(np.abs(diag_classifier.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.pearsonr(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find responsible neuron\n",
    "resp_neuron = np.argmax(np.abs(diag_classifier.coef_))\n",
    "print(resp_neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in np.argsort(np.abs(diag_classifier.coef_))[-10:]:\n",
    "    plot_colored_text(train_texts[:500], train_hiddens[:500, i],\n",
    "                      title='Most Responsible Neuron {}'.format(i),\n",
    "                      save_file='plots/temp_{}.png'.format(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(200):\n",
    "    plot_colored_text(train_texts[:500], train_hiddens[:500, i],\n",
    "                      title='Most Responsible Neuron {}'.format(i),\n",
    "                      save_file='plots/temp_{}.png'.format(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred = validate_hypothesis(model, LinearRegression(), hypothesis_inlinecounter,\n",
    "                                     train_len=95, test_len=5,\n",
    "                                     save_hyp='plots/hyp_inlinecounter_shake.png',\n",
    "                                     save_diag='plots/diag_inlinecounter_shake.png',\n",
    "                                     save_resp='plots/resp_inlinecounter_shake.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for model_name, model in [('shake_lstm', model_lstm), ('shake_gru', model_gru)]:\n",
    "    print(\"MODEL: \", model_name)\n",
    "    \n",
    "    full_test(model, hypothesis_capswords, 'Capitalized Words',\n",
    "              train_len=95, test_len=5, ex_name='{}_capswords'.format(model_name))\n",
    "    full_test(model, hypothesis_nouns, 'Nouns',\n",
    "              train_len=95, test_len=5, ex_name='{}_nouns'.format(model_name))\n",
    "    full_test(model, hypothesis_verbs, 'Verbs',\n",
    "              train_len=95, test_len=5, ex_name='{}_verbs'.format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
